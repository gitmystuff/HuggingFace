{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/HuggingFace/blob/main/Diffusers_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusers Starter"
      ],
      "metadata": {
        "id": "3gRpcTCrcQaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the Hugging Face `diffusers` Library?**\n",
        "\n",
        "The `diffusers` library is a Python library developed by Hugging Face that simplifies the process of working with diffusion models. It provides:\n",
        "\n",
        "* **Pre-trained Models:** A collection of ready-to-use diffusion models for various tasks (image generation, audio generation, etc.).\n",
        "* **Schedulers:** Implementations of different noise scheduling algorithms used in diffusion models.\n",
        "* **Pipelines:** High-level abstractions that combine models and schedulers for easy inference.\n",
        "* **Modularity:** The ability to access and manipulate individual components (models, schedulers) for custom diffusion system development.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1.  **Models:**\n",
        "    * These are the neural networks that learn to denoise images (or other data).\n",
        "    * They are trained to predict the noise added to an image at each step of the diffusion process.\n",
        "    * The example mentions `ddpm-celebahq-256`, which is a pre-trained model for generating celebrity images.\n",
        "2.  **Schedulers:**\n",
        "    * Schedulers define the noise schedule, which controls how much noise is added or removed at each step of the diffusion process.\n",
        "    * Different schedulers can lead to different generation speeds and quality.\n",
        "    * They are the logic that determines the amount of noise to add during the forward diffusion process, and the logic that determines how to remove the noise during the reverse diffusion process.\n",
        "3.  **Pipelines:**\n",
        "    * Pipelines bundle models and schedulers together, providing a simple interface for inference.\n",
        "    * The `DDPMPipeline` is an example of a pipeline that uses a DDPM (Denoising Diffusion Probabilistic Model) model.\n",
        "    * Pipelines are designed to make it very easy to go from zero to image generation, or other forms of generated data, very quickly.\n",
        "\n",
        "**Understanding Diffusion Models:**\n",
        "\n",
        "* **Forward Diffusion (Adding Noise):**\n",
        "    * The process of gradually adding noise to an image until it becomes pure noise.\n",
        "    * This process is typically performed in a series of discrete steps.\n",
        "* **Reverse Diffusion (Denoising):**\n",
        "    * The process of iteratively removing noise from a noisy image to generate a clean image.\n",
        "    * This is the generative process, where the model learns to reverse the forward diffusion.\n",
        "    * The model predicts the noise, and then substracts that predicted noise from the noisy image.\n",
        "* **Iterative Refinement:**\n",
        "    * Diffusion models generate images through multiple steps, allowing for gradual refinement and high-quality results.\n",
        "    * This is a key advantage over single-pass generative models like GANs.\n",
        "\n",
        "**Example Breakdown:**\n",
        "\n",
        "1.  **Loading a Pre-Trained Model:**\n",
        "    * The example uses `DDPMPipeline.from_pretrained(\"ddpm-celebahq-256\")` to load a pre-trained DDPM model.\n",
        "    * This model has been trained on the CelebA-HQ dataset, so it can generate realistic celebrity faces.\n",
        "2.  **Using the Pipeline:**\n",
        "    * The pipeline handles the entire diffusion process, from generating initial noise to denoising it into an image.\n",
        "    * The pipeline hides the complexity of the for loop that is used to iteratively denoise the image.\n",
        "\n",
        "**Deconstructing a Basic Pipeline:**\n",
        "\n",
        "* The library allows you to access the individual components of a pipeline (model and scheduler).\n",
        "* This enables you to customize the diffusion process, such as using a different scheduler or modifying the model.\n",
        "* This ability to access the individual components, is what gives the library its flexibility.\n",
        "* This flexibility is what allows researchers and developers to create new and innovative diffusion based systems.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The Hugging Face `diffusers` library provides a user-friendly and modular toolkit for working with diffusion models. It simplifies the process of loading pre-trained models, using pipelines for inference, and customizing diffusion systems. Its focus on modularity allows for both rapid prototyping and in depth research.\n"
      ],
      "metadata": {
        "id": "_tgFiWwJq6tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nvidia-smi"
      ],
      "metadata": {
        "id": "amAAklW7_Ypv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# def get_image_size(image_path):\n",
        "#     \"\"\"Gets the dimensions and file size of an image.\n",
        "\n",
        "#     Args:\n",
        "#         image_path: Path to the image file.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         width, height = img.size\n",
        "#         import os\n",
        "#         file_size = os.path.getsize(image_path)\n",
        "\n",
        "#         print(f\"Dimensions: {width} x {height} pixels\")\n",
        "#         print(f\"File size: {file_size} bytes\")\n",
        "\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Error: Image not found at {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "\n",
        "# image_file = \"cat playing.jpg\"\n",
        "# get_image_size(image_file)"
      ],
      "metadata": {
        "id": "5q-mY40LXQnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# def resize_image(image_path, output_path, size=(1024, 1024)):\n",
        "#     \"\"\"Resizes an image to a specified size using PIL.\n",
        "\n",
        "#     Args:\n",
        "#         image_path: Path to the input image.\n",
        "#         output_path: Path to save the resized image.\n",
        "#         size: Tuple representing the desired width and height (e.g., (1024, 1024)).\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         img_resized = img.resize(size, Image.Resampling.LANCZOS) # Use LANCZOS for high-quality downsampling\n",
        "#         img_resized.save(output_path)\n",
        "#         print(f\"Image resized and saved to {output_path}\")\n",
        "\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Error: Image not found at {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "\n",
        "# # Example usage:\n",
        "# input_image = \"cat playing.jpg\"  # Replace with your input image path\n",
        "# output_image = \"cat_playing_1024.jpg\" #Replace with desired output path and filename.\n",
        "\n",
        "# resize_image(input_image, output_image)\n",
        "\n",
        "# # Example with PNG:\n",
        "# # input_image_png = \"input.png\"\n",
        "# # output_image_png = \"output_1024x1024.png\"\n",
        "\n",
        "# # resize_image(input_image_png, output_image_png)"
      ],
      "metadata": {
        "id": "7d1Um3RsXIZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38f3cf1-eee6-4986-8ce0-362e5ca4cadf"
      },
      "source": [
        "# Working with Images in NumPy\n",
        "\n",
        "NumPy is a powerful library for numerical operations in Python, and it can also handle images stored as arrays. In this quick lecture, we'll cover how to work with images using NumPy. Although we'll primarily use OpenCV to open and view images, we'll revisit NumPy and Matplotlib later in the Deep Learning section.\n",
        "\n",
        "## Loading and Displaying Images with PIL and NumPy\n",
        "\n",
        "First, let's start by loading an image using the `PIL` library and converting it to a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81109201-7bbf-4fd7-9517-334515a01a8a"
      },
      "outputs": [],
      "source": [
        "# code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type of the image\n"
      ],
      "metadata": {
        "id": "jv8sGzgyYYx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbf0368f-d18f-42db-8ba9-9686931414b9"
      },
      "outputs": [],
      "source": [
        "# Convert image to NumPy array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8493cfaf-7e07-4b1b-b144-a1d715facb91"
      },
      "outputs": [],
      "source": [
        "# show new pic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc2622e1-7731-4b20-9f39-2e4df0559d5b"
      },
      "outputs": [],
      "source": [
        "# Copy the array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use cv2"
      ],
      "metadata": {
        "id": "IVgwbvI_aL4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c501b66-2615-4c8e-a703-27300476152f"
      },
      "outputs": [],
      "source": [
        "# Convert BGR to RGB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4b3e95b-fcc0-4c58-b204-53865f7a2603"
      },
      "outputs": [],
      "source": [
        "# Load image in grayscale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0035cc91-42ff-4dba-8a4c-31b32f14e92f"
      },
      "outputs": [],
      "source": [
        "# Resize image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Kernels"
      ],
      "metadata": {
        "id": "OV1E9kpHeFji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def apply_convolution(image_path, kernel):\n",
        "#     \"\"\"Applies a convolution to an image using a given kernel.\n",
        "\n",
        "#     Args:\n",
        "#         image_path: Path to the input image.\n",
        "#         kernel: NumPy array representing the convolution kernel.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         img = cv2.imread(image_path)\n",
        "#         if img is None:\n",
        "#             print(f\"Error: Could not read image at {image_path}\")\n",
        "#             return\n",
        "\n",
        "#         img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "#         convolved_img = cv2.filter2D(img_rgb, -1, kernel)  # Apply convolution\n",
        "\n",
        "#         plt.figure(figsize=(10, 5))\n",
        "\n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.imshow(img_rgb)\n",
        "#         plt.title(\"Original Image\")\n",
        "#         plt.axis('off')\n",
        "\n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.imshow(convolved_img)\n",
        "#         plt.title(\"Convolved Image\")\n",
        "#         plt.axis('off')\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "UZXX4i9nb-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sharpening"
      ],
      "metadata": {
        "id": "rBls-uQteI34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage:\n",
        "# image_file = \"cat_playing_1024.jpg\" #replace with your image.\n",
        "\n",
        "# # 1. Sharpening kernel:\n",
        "# sharpening_kernel = np.array([[-1, -1, -1],\n",
        "#                                [-1, 9.5, -1],\n",
        "#                                [-1, -1, -1]])\n",
        "\n",
        "\n",
        "# apply_convolution(image_file, sharpening_kernel)"
      ],
      "metadata": {
        "id": "h_64iV_Fdpr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Understanding the Sharpening Kernel:**\n",
        "\n",
        "```python\n",
        "sharpening_kernel = np.array([[-1, -1, -1],\n",
        "                               [-1, 9.5, -1],\n",
        "                               [-1, -1, -1]])\n",
        "```\n",
        "\n",
        "* **NumPy Array:** This creates a 3x3 matrix (a kernel) using the NumPy library.\n",
        "* **Kernel Structure:**\n",
        "    * The center value is `9.5`. This is the most important part. It emphasizes the central pixel's value.\n",
        "    * The surrounding values are all `-1`. These are negative values that subtract from the surrounding pixels.\n",
        "* **How it Works (Convolution):**\n",
        "    * When this kernel is applied to an image, it performs a weighted sum of the pixels in a 3x3 neighborhood.\n",
        "    * The central pixel's value is multiplied by `9.5`, while the surrounding pixels' values are multiplied by `-1`.\n",
        "    * This process enhances the differences between the central pixel and its neighbors.\n",
        "    * Where there are sharp transitions (edges), the difference between the center and surrounding pixels will be large, and the result will be a larger value. This is why edges are enhanced.\n",
        "    * Essentially, the kernel is subtracting a slightly blurred version of the image from the original image.\n",
        "\n",
        "**2. Why Sharpening (and Similar Filters) Are Used in CNNs:**\n",
        "\n",
        "* **Feature Extraction:**\n",
        "    * CNNs are designed to learn features from images. These features can be edges, corners, textures, and other patterns.\n",
        "    * Convolutional layers in CNNs use kernels (filters) to extract these features.\n",
        "    * Sharpening kernels, edge detection kernels, and blur kernels are examples of basic filters that can extract specific types of features.\n",
        "* **Edge Enhancement:**\n",
        "    * Edges are fundamental features in images. They define the boundaries of objects and provide crucial information for object recognition.\n",
        "    * Sharpening kernels enhance edges, making them more distinct. This can improve the performance of CNNs in tasks like object detection and image classification.\n",
        "* **Learning Kernels:**\n",
        "    * In a typical CNN, the kernel values are not fixed. Instead, they are learned during the training process.\n",
        "    * The network adjusts the kernel values to minimize the error between its predictions and the actual labels.\n",
        "    * This allows the network to learn kernels that are optimal for the specific task.\n",
        "    * While we manually set the sharpening kernel's values in our example, a CNN would learn what values are best.\n",
        "* **Early Layers:**\n",
        "    * Sharpening and edge detection kernels are often used in the early layers of CNNs.\n",
        "    * These layers are responsible for extracting low-level features, such as edges and textures.\n",
        "    * The later layers of the network then combine these low-level features to form more complex representations of objects.\n",
        "* **Data Preprocessing:**\n",
        "    * Sometimes sharpening can be used as a preprocessing step, before the image is inputted into the CNN. This is done to help the CNN find the edges within the image easier.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Sharpening kernels are used in CNNs (either as fixed filters or learned kernels) to enhance edges and extract relevant features from images. This improves the network's ability to recognize objects and patterns.\n"
      ],
      "metadata": {
        "id": "tRthk3lTeBxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edge Detection"
      ],
      "metadata": {
        "id": "IPOxIq22eSR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Edge detection kernel:\n",
        "# edge_detection_kernel = np.array([[-1, -1, -1],\n",
        "#                                   [0, 0, 0],\n",
        "#                                   [1, 1, 1]])\n",
        "\n",
        "\n",
        "# apply_convolution(image_file, edge_detection_kernel)"
      ],
      "metadata": {
        "id": "MsJXd_FOclcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blurring"
      ],
      "metadata": {
        "id": "D7hbhxcYeXoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. Blur kernel (average):\n",
        "# blur_kernel = np.ones((15, 15), np.float32) / 225\n",
        "\n",
        "# apply_convolution(image_file, blur_kernel)\n"
      ],
      "metadata": {
        "id": "dbmH_5AJct0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNs\n",
        "\n",
        "* https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/\n",
        "* https://g.co/gemini/share/e9ebc46308d8  "
      ],
      "metadata": {
        "id": "IgvjZPPBfR6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from diffusers import DDPMPipeline\n",
        "\n",
        "# ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")\n",
        "# image = ddpm(num_inference_steps=30).images[0]\n",
        "# image"
      ],
      "metadata": {
        "id": "dmkRaSwDrQXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you see \"DDPM,\" it refers to \"Denoising Diffusion Probabilistic Models.\" Here's a breakdown of what that means in the context of image generation and the code you provided:\n",
        "\n",
        "**Denoising Diffusion Probabilistic Models (DDPMs):**\n",
        "\n",
        "* **Core Idea:**\n",
        "    * DDPMs are a type of generative model that learn to create images (or other data) by reversing a process that gradually adds noise to the data.\n",
        "    * Imagine starting with a clear image and progressively adding more and more noise until it becomes pure random noise.\n",
        "    * A DDPM learns to reverse this process, starting with random noise and gradually removing the noise to reconstruct a clear image.\n",
        "* **Key Processes:**\n",
        "    * **Forward Diffusion:**\n",
        "        * This is the process of adding noise to the image over a series of steps.\n",
        "        * Each step adds a little more Gaussian noise, eventually turning the image into pure noise.\n",
        "    * **Reverse Diffusion:**\n",
        "        * This is the generative process.\n",
        "        * The model learns to predict and remove the noise at each step, gradually revealing the underlying image.\n",
        "        * This process is iterative, meaning it repeats many times to refine the image.\n",
        "\n",
        "**Understanding `image = ddpm(num_inference_steps=30).images[0]`:**\n",
        "\n",
        "* **`ddpm`:**\n",
        "    * This represents a DDPM pipeline or model that has been loaded, likely from the Hugging Face `diffusers` library.\n",
        "    * It encapsulates the model and the scheduler needed to perform the diffusion process.\n",
        "* **`num_inference_steps=30`:**\n",
        "    * This parameter specifies the number of steps to take during the reverse diffusion process (denoising).\n",
        "    * A higher number of steps generally leads to higher-quality images but takes longer to generate.\n",
        "    * in this case, the model will run the reverse diffusion process 30 times.\n",
        "* **.images[0]`:**\n",
        "    * The `ddpm` pipeline, when executed, produces a set of generated images.\n",
        "    * `.images` is accessing the array of generated images.\n",
        "    * `[0]` selects the first image from that set.\n",
        "* **`image`:**\n",
        "    * The variable `image` now holds the generated image data, which can then be displayed or further processed.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "The code tells the DDPM model to:\n",
        "\n",
        "1.  Start with random noise.\n",
        "2.  Perform 30 steps of denoising.\n",
        "3.  Give me the resulting image.\n",
        "\n",
        "Therefore, the variable \"image\" will contain the generated image that was created by the DDPM model.\n"
      ],
      "metadata": {
        "id": "uDeEJf-Trshd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down how a diffusion pipeline works, focusing on the `UNet2DModel` and `DDPMScheduler`, and then explain how to reconstruct that process manually.\n",
        "\n",
        "**Understanding the Pipeline (Simplified):**\n",
        "\n",
        "Imagine you have a blurry image that you want to sharpen. A diffusion pipeline does something similar, but it starts with *pure noise* and gradually turns it into a clear image.\n",
        "\n",
        "Here's a rephrased explanation:\n",
        "\n",
        "\"The pipeline is a pre-built system that generates images by repeatedly removing noise. It uses two main components:\n",
        "\n",
        "1.  **`UNet2DModel` (The 'Painter'):** This is the core neural network. It's like an artist who can look at a noisy image and figure out what the noise looks like. It essentially predicts the 'direction' we need to move in to get a less noisy image.\n",
        "2.  **`DDPMScheduler` (The 'Guide'):** This component controls the denoising process. It tells the 'painter' how much noise to remove at each step. It's like a guide that says, 'Remove this much noise now, then this much next time.'\n",
        "\n",
        "Here's how they work together:\n",
        "\n",
        "* Start with random noise (a completely blurry image).\n",
        "* The `UNet2DModel` analyzes the noise and estimates what the noise looks like (the 'noise residual').\n",
        "* The `DDPMScheduler` uses this estimate to calculate a slightly less noisy version of the image.\n",
        "* This process repeats many times. Each time, the image gets a little less noisy and more detailed.\n",
        "* After a certain number of steps, the image is clear.\"\n",
        "\n",
        "**In more technical terms:**\n",
        "\n",
        "* The `UNet2DModel` is a specific type of neural network architecture designed for image processing. It's particularly good at finding patterns in noisy images.\n",
        "* The `DDPMScheduler` implements the noise schedule, which is a key part of the diffusion process. It dictates how much noise is added or removed at each timestep.\n",
        "* The term \"noise residual\" refers to the model's prediction of the noise that was added at that given timestep.\n",
        "\n",
        "**Recreating the Pipeline Manually:**\n",
        "\n",
        "The example then shows how to do the same thing as the pipeline, but by loading the `UNet2DModel` and `DDPMScheduler` directly and writing the denoising loop yourself. This gives you more control over the process.\n",
        "\n",
        "**Why do this?**\n",
        "\n",
        "* **Understanding:** It helps you understand how the pipeline works under the hood.\n",
        "* **Customization:** It allows you to modify the denoising process, such as using a different scheduler or changing the number of steps.\n",
        "* **Research:** It's essential for developing new diffusion techniques.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The pipeline simplifies the process of generating images with diffusion models. By manually reconstructing the pipeline, you gain a deeper understanding of the underlying mechanisms and unlock greater flexibility.\n"
      ],
      "metadata": {
        "id": "1S5tXY_HsUvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from diffusers import UNet2DModel, DDPMScheduler\n",
        "\n",
        "# repo_id = \"google/ddpm-church-256\"\n",
        "# scheduler = DDPMScheduler.from_pretrained(repo_id)\n",
        "# model = UNet2DModel.from_pretrained(repo_id, device=\"cuda\")"
      ],
      "metadata": {
        "id": "_N1Fd5ItuYbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler.set_timesteps(50)\n",
        "# scheduler.timesteps"
      ],
      "metadata": {
        "id": "mRptNP0xuod6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VdHRp1UQsaIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# sample_size = model.config.sample_size\n",
        "# noise = torch.randn((1, 3, sample_size, sample_size))\n",
        "\n",
        "# for t in scheduler.timesteps:\n",
        "#     with torch.no_grad():\n",
        "#         noisy_residual = model(noise, t).sample\n",
        "\n",
        "#     previous_noisy_sample = scheduler.step(noisy_residual, t, noise).prev_sample\n",
        "#     noise = previous_noisy_sample"
      ],
      "metadata": {
        "id": "uFe_EQlfu7FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Breakdown:**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "sample_size = model.config.sample_size\n",
        "noise = torch.randn((1, 3, sample_size, sample_size))\n",
        "\n",
        "for t in scheduler.timesteps:\n",
        "    with torch.no_grad():\n",
        "        noisy_residual = model(noise, t).sample\n",
        "\n",
        "    previous_noisy_sample = scheduler.step(noisy_residual, t, noise).prev_sample\n",
        "    noise = previous_noisy_sample\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1.  **`import torch`:**\n",
        "    * Imports the PyTorch library, which is essential for tensor operations and neural network computations.\n",
        "\n",
        "2.  **`sample_size = model.config.sample_size`:**\n",
        "    * Retrieves the size of the images that the model is designed to generate.\n",
        "    * `model.config` accesses the configuration settings of the `UNet2DModel`.\n",
        "    * `sample_size` stores the width and height of the image (assuming square images).\n",
        "\n",
        "3.  **`noise = torch.randn((1, 3, sample_size, sample_size))`:**\n",
        "    * Creates a tensor filled with random Gaussian noise.\n",
        "    * `(1, 3, sample_size, sample_size)` defines the shape of the tensor:\n",
        "        * `1`: Batch size (generating a single image).\n",
        "        * `3`: Number of color channels (RGB).\n",
        "        * `sample_size, sample_size`: Width and height of the image.\n",
        "    * This represents the starting point of the denoising process—a completely noisy image.\n",
        "\n",
        "4.  **`for t in scheduler.timesteps:`:**\n",
        "    * Starts a loop that iterates through the timesteps defined by the `DDPMScheduler`.\n",
        "    * Each timestep `t` represents a specific level of noise in the diffusion process.\n",
        "    * `scheduler.timesteps` is an array of timesteps that the scheduler uses.\n",
        "\n",
        "5.  **`with torch.no_grad():`:**\n",
        "    * Disables gradient calculations within the block.\n",
        "    * This is done because we're performing inference (generating an image), not training the model. We don't need to compute gradients.\n",
        "    * This saves memory and speeds up the process.\n",
        "\n",
        "6.  **`noisy_residual = model(noise, t).sample`:**\n",
        "    * Performs a forward pass through the `UNet2DModel`.\n",
        "    * `noise`: The current noisy image.\n",
        "    * `t`: The current timestep.\n",
        "    * `model(noise, t)`: Passes the noise and the timestep to the model.\n",
        "    * `.sample`: Retrieves the model's prediction of the noise residual (the estimated noise that was added at this timestep).\n",
        "\n",
        "7.  **`previous_noisy_sample = scheduler.step(noisy_residual, t, noise).prev_sample`:**\n",
        "    * Uses the `DDPMScheduler` to calculate the less noisy image at the previous timestep.\n",
        "    * `scheduler.step(noisy_residual, t, noise)`:\n",
        "        * `noisy_residual`: The model's noise prediction.\n",
        "        * `t`: The current timestep.\n",
        "        * `noise`: The current noisy image.\n",
        "        * This function applies the denoising step, using the noise prediction and the scheduler's logic.\n",
        "    * `.prev_sample`: Extracts the denoised image from the scheduler's output.\n",
        "\n",
        "8.  **`noise = previous_noisy_sample`:**\n",
        "    * Updates the `noise` variable with the denoised image.\n",
        "    * This denoised image becomes the input for the next iteration of the loop.\n",
        "\n",
        "**In essence, this code:**\n",
        "\n",
        "* Starts with pure noise.\n",
        "* Iteratively removes noise using the `UNet2DModel` and `DDPMScheduler`.\n",
        "* Repeats this process for each timestep, gradually generating a clearer image.\n",
        "* The for loop, is the reverse diffusion process.\n",
        "* The model predicts the noise, and the scheduler uses that prediction to take a step backwards in the noise schedule.\n",
        "* The final noise variable, after the for loop completes, will contain the generated image.\n"
      ],
      "metadata": {
        "id": "Wn_ZnUBdxD0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "\n",
        "# # Normalize the image data\n",
        "# image = (noise / 2 + 0.5).clamp(0, 1).squeeze()\n",
        "# # Change the shape and type for image conversion\n",
        "# image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
        "\n",
        "# image = Image.fromarray(image)\n",
        "# image"
      ],
      "metadata": {
        "id": "NFZwLz2twoMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import torch\n",
        "# from transformers import CLIPTextModel, CLIPTokenizer\n",
        "# from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n",
        "# tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
        "# text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True)\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True)"
      ],
      "metadata": {
        "id": "DYiplOQGy6e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code snippet sets up the core components of a Stable Diffusion model, a popular text-to-image generation system. Let's break it down line by line:\n",
        "\n",
        "**1. Importing Libraries:**\n",
        "\n",
        "```python\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "```\n",
        "\n",
        "* **`from PIL import Image`**: Imports the `Image` module from the Pillow (PIL) library, used for working with image files (loading, saving, displaying).\n",
        "* **`import torch`**: Imports the PyTorch library, a deep learning framework used for building and running neural networks.\n",
        "* **`from transformers import CLIPTextModel, CLIPTokenizer`**: Imports specific classes from the Hugging Face `transformers` library:\n",
        "    * `CLIPTextModel`: Represents the text encoder part of the CLIP (Contrastive Language–Image Pre-training) model, which converts text prompts into numerical embeddings.\n",
        "    * `CLIPTokenizer`: Used to tokenize text prompts, breaking them down into smaller units (tokens) that the CLIP model can understand.\n",
        "* **`from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler`**: Imports classes from the Hugging Face `diffusers` library, designed for diffusion models:\n",
        "    * `AutoencoderKL`: Represents the variational autoencoder (VAE), which compresses images into a lower-dimensional latent space and reconstructs them.\n",
        "    * `UNet2DConditionModel`: Represents the U-Net, the core neural network that iteratively denoises latent representations to generate images.\n",
        "    * `PNDMScheduler`: Implements the PNDM (Pseudo Numerical Methods for Diffusion Models) scheduler, which controls the denoising process.\n",
        "\n",
        "**2. Loading Pre-trained Models:**\n",
        "\n",
        "```python\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True)\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True)\n",
        "```\n",
        "\n",
        "* **`from_pretrained(\"CompVis/stable-diffusion-v1-4\", ...)`**: This is the key function from the Hugging Face libraries. It loads pre-trained model weights from the specified repository (\"CompVis/stable-diffusion-v1-4\" in this case).\n",
        "* **`subfolder=\"vae\"`, `subfolder=\"tokenizer\"`, `subfolder=\"text_encoder\"`, `subfolder=\"unet\"`**: These arguments specify which subfolder within the repository to load the model from. This is how Stable Diffusion is split into its component parts.\n",
        "* **`use_safetensors=True`**: This is a security and efficiency measure. Safetensors are a safe way to store and load tensors. It is the new standard for model weights.\n",
        "\n",
        "**In essence, this code does the following:**\n",
        "\n",
        "1.  **Imports necessary libraries** for image processing, deep learning, and diffusion models.\n",
        "2.  **Downloads and loads the pre-trained weights** for the VAE, tokenizer, text encoder, and U-Net components of Stable Diffusion from the Hugging Face Model Hub.\n",
        "\n",
        "These loaded components are the building blocks for generating images from text prompts. The subsequent steps in a Stable Diffusion pipeline would typically involve:\n",
        "\n",
        "* Tokenizing and encoding the text prompt using the `tokenizer` and `text_encoder`.\n",
        "* Generating a noisy latent representation.\n",
        "* Iteratively denoising the latent representation using the `unet` and `PNDMScheduler`.\n",
        "* Decoding the final latent representation into an image using the `vae`.\n"
      ],
      "metadata": {
        "id": "uYQ5w3FX3Nby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from diffusers import UniPCMultistepScheduler\n",
        "\n",
        "# scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
        "\n",
        "# torch_device = \"cuda\"\n",
        "# vae.to(torch_device)\n",
        "# text_encoder.to(torch_device)\n",
        "# unet.to(torch_device)"
      ],
      "metadata": {
        "id": "DodYGh8ZzFOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = [\"cartoon cat playing upright bass\"]\n",
        "# height = 512  # default height of Stable Diffusion\n",
        "# width = 512  # default width of Stable Diffusion\n",
        "# num_inference_steps = 25  # Number of denoising steps\n",
        "# guidance_scale = 7.5  # Scale for classifier-free guidance"
      ],
      "metadata": {
        "id": "t1oeKg0UypaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = len(prompt)\n",
        "\n",
        "# text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "# max_length = text_input.input_ids.shape[-1]\n",
        "# uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "# uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "# text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ],
      "metadata": {
        "id": "Bd4TQFjVyxHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# latents = torch.randn((batch_size, unet.config.in_channels, height // 8, width // 8), device=torch_device)\n",
        "# latents = latents * scheduler.init_noise_sigma\n",
        "\n",
        "# scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# for t in tqdm(scheduler.timesteps):\n",
        "#     latent_model_input = torch.cat([latents] * 2)\n",
        "#     latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "#     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "#     noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "#     latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ],
      "metadata": {
        "id": "tG96hpm7zf5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# latents = 1/ 0.18215 * latents\n",
        "# with torch.no_grad():\n",
        "#     image = vae.decode(latents).sample\n",
        "\n",
        "# image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
        "# image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
        "# image = Image.fromarray(image)\n",
        "# image"
      ],
      "metadata": {
        "id": "gFkk3DYwzwwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Concepts\n",
        "\n",
        "* Diffusers Library: This is a library from Hugging Face designed to make it easier to work with diffusion-based text-to-image models.\n",
        "   \n",
        "* Image Data Representation: Images are fundamentally represented as arrays of pixels. Grayscale images use 2D arrays, with pixel values ranging typically from 0 to 1 or 0 to 255. Color images use the RGB model, where each pixel is defined by a combination of red, green, and blue values.\n",
        "   \n",
        "* Color Channels: The RGB color model uses red, green, and blue channels. Additionally, an optional fourth channel, alpha, is sometimes used to represent transparency.\n",
        "   \n",
        "* NumPy for Image Data: NumPy is a Python library that can be used to process image data, including converting images into arrays and manipulating color channels.\n",
        "   \n",
        "* OpenCV (cv2): OpenCV is a library used for image processing, capable of loading and displaying images. It's important to note that OpenCV uses BGR color order by default, whereas matplotlib uses RGB.\n",
        "   \n",
        "* Image Generation Models: These are models that take a text string as input and generate an image as output.\n",
        "   \n",
        "* Diffusion Process: This is a technique used by image generation models to create images from noisy inputs by reversing a corruption process.\n",
        "   \n",
        "* Text Embedding: This refers to the creation of a vector representation of text, which is used in the early stages of image generation.\n",
        "   \n",
        "* CLIP (Contrastive Language-Image Pre-training): CLIP is a model designed to understand the relationship between text and images. It is trained to maximize similarity between correct image-caption pairs and minimize similarity between incorrect pairs.\n",
        "   \n",
        "* Decoder (Diffusion Model/Unclip): This component generates an image from a prior embedding by reversing a noising process.\n",
        "   \n",
        "* Auto Pipelines: This is a feature of the Diffusers library that simplifies the process of loading and using pre-trained models by attempting to automatically detect the correct internal pipelines."
      ],
      "metadata": {
        "id": "OQmA1gWRyreD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffuser Process\n",
        "\n",
        "Here is an explanation of the diffuser process using bullet points:\n",
        "\n",
        "* The diffusion model is trained to reverse a fixed corruption process.\n",
        "   \n",
        "* The corruption process involves adding small amounts of Gaussian noise to an image, gradually erasing information from it.\n",
        "   \n",
        "* By the final step of this process, the image becomes indistinguishable from pure noise.\n",
        "   \n",
        "* The diffusion model is trained to reverse this process, learning to regenerate the information that was erased at each step.\n",
        "   \n",
        "* In simpler terms, the model starts with a noisy image and works to remove the noise, aiming to produce an image that aligns with the given text embedding.\n",
        "   \n",
        "* There are two main stages in this process:\n",
        "   * The prior stage generates a clip image embedding, describing the gist of an image from the given caption, which is itself a text embedding.\n",
        "   * The decoder stage, also known as Unclip, is the diffusion model that generates the image from this embedding.\n",
        "   \n",
        "* The Unclip or decoder receives a corrupted version of the image it’s trained to reconstruct, along with the clip image embedding of the clean image.\n",
        "   \n",
        "* After these two stages, up-sampling may be performed on the image to achieve higher resolution.\n",
        "   \n",
        "* Early versions of stable diffusion were trained on 512x512 pixel images, so higher resolution outputs used a separate model to upscale the image.\n",
        "   \n",
        "* Latest models are trained on larger images, like 1024x1024 pixels, and can upscale to 4K or 8K resolutions."
      ],
      "metadata": {
        "id": "TkgdZgV63V0k"
      }
    }
  ]
}