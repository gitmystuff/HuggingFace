{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/HuggingFace/blob/main/Audio_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio"
      ],
      "metadata": {
        "id": "ixFfbwmJm2Cf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac67eb13-b1a5-4682-a0b6-dc949c60a45f"
      },
      "outputs": [],
      "source": [
        "# import librosa\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio file\n"
      ],
      "metadata": {
        "id": "oa-pqIN6U56F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2831d6f6-800a-4091-a4db-9b72066f61ee"
      },
      "outputs": [],
      "source": [
        "# Play the loaded audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d835e594-af71-4426-90f9-cb4de681607c"
      },
      "outputs": [],
      "source": [
        "# plot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# space for audio"
      ],
      "metadata": {
        "id": "4Nd6SjdGZOrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Axes:**\n",
        "\n",
        "* **X-axis (Sample):** This represents the progression of time, but instead of seconds or milliseconds, it's measured in \"samples.\"  A \"sample\" is a discrete measurement of the audio signal's amplitude taken at regular intervals. The sampling rate (measured in Hertz or kHz) tells you how many samples are taken per second. So, moving along the x-axis means you're moving forward in time.\n",
        "* **Y-axis (Amplitude):** This represents the instantaneous amplitude (or strength) of the audio signal at each specific sample point. Amplitude corresponds to the displacement of the sound wave. In simpler terms, it's related to the \"loudness\" of the sound at that moment. The higher the amplitude (positive or negative), the stronger the signal.\n",
        "\n",
        "**Interpreting the Waveform:**\n",
        "\n",
        "* **Shape and Variation:** The blue line shows how the amplitude of the audio signal changes over time.  Notice the complex and irregular shape of the wave. This is typical of real-world audio, which is rarely a simple, smooth sine wave.\n",
        "* **Silence/Quiet vs. Loud/Intense:**\n",
        "    * **Areas of High Density (Vertical Clumping):** Where the blue line is very dense vertically (lots of rapid changes in amplitude), this indicates a portion of the audio with a strong signal â€“ likely a louder or more intense sound.  For example, look at the regions around samples 0-150000, 225000-350000.\n",
        "    * **Areas of Low Density (Spread Out):** Where the blue line is more spread out vertically (smaller changes in amplitude), this indicates a quieter or less intense portion of the audio.  For example, the region around sample 200000 shows a noticeable drop in amplitude, suggesting a pause or quieter sound.\n",
        "* **Sudden Changes:** Sharp, rapid changes in the waveform indicate sudden changes in the audio signal. For instance, the spike around sample 200000 suggests a very abrupt sound event.\n",
        "* **Repetitive Patterns:** While this waveform is complex, you might notice some repeating patterns or sections with similar characteristics.  These could relate to specific frequencies or rhythmic elements within the audio.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This waveform provides a visual representation of how the \"loudness\" of an audio signal changes over time. By analyzing the density, height, and shape of the wave, you can gain insights into the characteristics of the sound it represents.  It's a foundational tool for understanding and processing audio data."
      ],
      "metadata": {
        "id": "87Axpr04W91H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76d6a5a5-f009-4927-a6dc-f026e066e6e4"
      },
      "source": [
        "## Fast Fourier Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8a90efd-395e-4f7e-b372-200619a9f66b"
      },
      "outputs": [],
      "source": [
        "# samples to seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ac533b-9a8a-425e-a2d4-efd1bc2f3b77"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# dft_input = samples[:six_seconds]\n",
        "\n",
        "# # calculate the DFT\n",
        "# window = np.hanning(len(dft_input))\n",
        "# windowed_input = dft_input * window\n",
        "# dft = np.fft.rfft(windowed_input)\n",
        "\n",
        "# # get the amplitude spectrum in decibels\n",
        "# amplitude = np.abs(dft)\n",
        "# amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
        "\n",
        "# # get the frequency bins\n",
        "# frequency = librosa.fft_frequencies(sr=sr, n_fft=len(dft_input))\n",
        "\n",
        "# plt.figure(figsize=(10, 2),dpi=150)\n",
        "# plt.plot(frequency, amplitude_db)\n",
        "# plt.xlabel(\"Frequency (Hz)\")\n",
        "# plt.ylabel(\"Amplitude (dB)\")\n",
        "# plt.xscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Axes:**\n",
        "\n",
        "* **X-axis (Frequency):** This axis represents the frequency of the audio signal, measured in Hertz (Hz). Notice that the x-axis is on a logarithmic scale (10<sup>-1</sup>, 10<sup>0</sup>, 10<sup>1</sup>, 10<sup>2</sup>, 10<sup>3</sup>, 10<sup>4</sup>). This logarithmic scaling is very common in frequency plots because it allows us to visualize a wide range of frequencies, from very low to very high, in a more comprehensible way. It spreads out the lower frequencies, which are often perceptually more important in audio, and compresses the higher frequencies.\n",
        "* **Y-axis (Amplitude (dB)):** This axis represents the amplitude or strength of each frequency component, measured in decibels (dB). Decibels are a logarithmic unit that's used to express sound levels (and other signal strengths) in a way that aligns better with human perception of loudness.  A higher dB value indicates a stronger presence of that particular frequency in the audio signal.\n",
        "\n",
        "**Interpreting the Plot:**\n",
        "\n",
        "* **Frequency Content:** The plot shows the distribution of frequencies present in the audio signal. It tells you which frequencies are dominant and which are weaker.\n",
        "* **Dominant Frequencies:** Look for the peaks or higher points on the y-axis. These indicate frequencies with higher amplitudes, meaning they are more prominent in the audio.\n",
        "* **Frequency Range:** The plot shows the range of frequencies present in the signal. You can see the lowest and highest frequencies that have significant energy.\n",
        "* **Overall Shape:** The general shape of the curve provides insights into the overall tonal balance of the audio.\n",
        "* **Specific Observations:**\n",
        "    * **Low-Frequency Content:** The plot shows a relatively flat response at lower frequencies (below 10 Hz) around -18 dB. This means that the signal has a consistent, but relatively low, amount of energy in the very low-frequency range.\n",
        "    * **Mid-Frequency Rise:** There's a gradual rise in amplitude from the low frequencies up to around 10-20 Hz, indicating an increase in energy in this range.\n",
        "    * **Peak and Roll-Off:** There's a peak or a region of maximum amplitude in the mid-frequencies (around 10-20 Hz), and then a significant roll-off or decrease in amplitude as the frequency increases.\n",
        "    * **High-Frequency Attenuation:** The plot shows a steep decline in amplitude as frequency increases beyond 100 Hz. This means that the higher frequencies have much less energy than the lower and mid frequencies. The signal is significantly attenuated (reduced) in the higher frequency range.\n",
        "    * **Noise/Fluctuations:** The jaggedness of the line, especially at higher frequencies, can indicate noise or rapid variations in the frequency content.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This FFT plot reveals the frequency composition of the audio signal. It indicates that the signal has a concentration of energy in the lower and mid-frequency range, with a rapid decrease in energy at higher frequencies. This suggests that the audio might sound \"bassy\" or \"warm\" with less \"treble\" or \"brightness.\" The logarithmic frequency scale allows us to see both the detail in the lower frequencies and the overall trend across a wide range of frequencies."
      ],
      "metadata": {
        "id": "-arxD6e2Xvvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to locate the \"mid-frequencies\" on the x-axis of the FFT plot you provided:\n",
        "\n",
        "**Understanding \"Mid-Frequencies\" is Subjective**\n",
        "\n",
        "* There's no universally strict definition of exactly where \"low,\" \"mid,\" and \"high\" frequencies begin and end in audio. It's somewhat perceptual and depends on the context (e.g., the specific audio content, the human ear's sensitivity).\n",
        "* However, we can give some general guidelines.\n",
        "\n",
        "**Guidelines for Approximating Mid-Frequencies on Your Plot**\n",
        "\n",
        "Given the logarithmic scale of your x-axis, here's how to think about it:\n",
        "\n",
        "1.  **Identify the Overall Range:**\n",
        "    * Your x-axis goes from 10<sup>-1</sup> Hz (0.1 Hz) to 10<sup>4</sup> Hz (10,000 Hz). This is your total frequency range displayed.\n",
        "\n",
        "2.  **Divide Logarithmically (Roughly):**\n",
        "    * Since it's a log scale, try to divide the range in a log-proportional way, rather than linearly.\n",
        "    * A very rough approximation might be:\n",
        "        * **Low:** Around 10<sup>-1</sup> Hz (0.1 Hz) to 10<sup>1</sup> Hz (10 Hz)\n",
        "        * **Mid:** Around 10<sup>1</sup> Hz (10 Hz) to 10<sup>3</sup> Hz (1,000 Hz)\n",
        "        * **High:** Around 10<sup>3</sup> Hz (1,000 Hz) to 10<sup>4</sup> Hz (10,000 Hz)\n",
        "\n",
        "3.  **Refine Based on Audio Perception (More Subjective):**\n",
        "    * In audio, we often think of:\n",
        "        * **Bass:** Roughly up to 250 Hz\n",
        "        * **Midrange:** Roughly 250 Hz to 4 kHz (4,000 Hz)\n",
        "        * **Treble:** Roughly 4 kHz and up\n",
        "\n",
        "4.  **Apply to the Plot:**\n",
        "    * Based on these guidelines, on the plot, the **mid-frequencies** would roughly span from:\n",
        "        * Around 10<sup>1</sup> (10 Hz) to somewhere between 10<sup>2</sup> and 10<sup>3</sup> (100 Hz to 1000Hz)\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "* **Logarithmic Perception:** Remember that our ears perceive frequency logarithmically. This is why the log scale is helpful â€“ equal distances on the log scale correspond to roughly equal intervals in how we *hear* pitch.\n",
        "* **Context is Key:** What's considered \"mid\" can really change. In speech, the mid-range is crucial for intelligibility. In music, different genres emphasize different parts of the spectrum."
      ],
      "metadata": {
        "id": "QcPSdw7BYmhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Db comes down to **relative measurement** and **context**.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "**1. Digital Audio Processing (Like in the Plot)**\n",
        "\n",
        "* **Relative Decibels (dB):** In digital audio processing and analysis (as in the plot), decibels (dB) are often used to represent the amplitude of frequency components *relative* to some reference level *within the digital signal itself*.\n",
        "* **0 dB as Maximum:** In many digital audio contexts, 0 dB is defined as the *maximum possible amplitude* that the digital system can represent without clipping (distorting). Any signal that goes beyond 0 dB in the digital domain would be clipped.\n",
        "* **Negative dB Values:** Therefore, all other amplitudes within the digital signal are expressed as *negative* dB values, indicating how much *lower* they are compared to that maximum level.\n",
        "* **Normalization:** It's common to normalize digital audio, meaning scaling it so that the loudest part reaches 0 dB. This maximizes the use of the available digital range.\n",
        "\n",
        "**2. Stereo Receiver/Real-World Audio**\n",
        "\n",
        "* **Absolute Decibels (dB SPL):** In the real world, and with devices like stereo receivers, decibels are often used to measure *absolute* sound pressure levels (SPL).\n",
        "* **dB SPL Reference:** dB SPL is referenced to the threshold of human hearing (20 micropascals), which is considered 0 dB SPL.\n",
        "* **Positive dB Values:** Most sounds we hear have *positive* dB SPL values because they are louder than that threshold of hearing.\n",
        "* **Amplification:** A stereo receiver *amplifies* the incoming audio signal to increase its sound pressure level, making it loud enough for us to hear. The volume control adjusts this amplification.\n",
        "\n",
        "**Why the Difference?**\n",
        "\n",
        "* **Context and Purpose:** The plot is for *analyzing* the frequency content *within* a digital audio file or signal. It's about the relative strength of different frequencies *within* that recording. A stereo receiver is about taking a relatively weak signal and making it powerful enough to drive speakers and create sound in the real world.\n",
        "* **Representation:** The plot represents the signal in its *digital* form, before it's converted to an analog signal to be played through speakers. A stereo receiver deals with the analog signal and its physical sound pressure.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "* The plot uses dB to show the relative strength of frequencies *within* the digital signal, where 0 dB is the maximum.\n",
        "* A stereo receiver uses dB (often dB SPL) to show the absolute sound pressure level of the sound we hear in the real world, referenced to the threshold of hearing.\n",
        "\n",
        "It's all about what the 0 dB reference point means in each situation."
      ],
      "metadata": {
        "id": "VTOysKYeht0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Harmonics**\n",
        "\n",
        "* **Fundamental Frequency:** When a musical instrument (or any object) vibrates, it typically vibrates most strongly at a specific frequency, called the fundamental frequency. This is what we perceive as the \"pitch\" of the note.\n",
        "* **Harmonics (Overtones):** In addition to the fundamental frequency, the object also vibrates at multiples of that frequency. These multiples are called harmonics or overtones.\n",
        "    * The first harmonic is the fundamental frequency itself.\n",
        "    * The second harmonic is 2 times the fundamental frequency.\n",
        "    * The third harmonic is 3 times the fundamental frequency, and so on.\n",
        "* **Timbre:** Harmonics are crucial for creating the timbre or tone color of a sound. Different instruments produce different relative strengths of harmonics, which is why a violin sounds different from a flute even when playing the same note (fundamental frequency).\n",
        "\n",
        "**Relating to the FFT Plot**\n",
        "\n",
        "* **Frequency Content:** The FFT plot shows the amplitude of different frequencies present in the audio signal.\n",
        "* **Possible Harmonics:** If the audio signal contains a pitched sound, you might expect to see peaks in the FFT plot at the fundamental frequency and at its harmonic multiples.\n",
        "* **3 kHz and Above:** Therefore, the frequency content *above* 3 kHz *could* indeed represent harmonics, *if* the audio signal has a fundamental frequency and produces harmonics.\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Not Always Just Harmonics:** However, it's crucial to remember that the frequencies above 3 kHz in an FFT plot are *not necessarily* *exclusively* harmonics. They could also represent:\n",
        "    * **Noise:** Random fluctuations in the audio signal.\n",
        "    * **Other Sounds:** Other instruments, speech components, or environmental sounds present in the recording.\n",
        "    * **High-Frequency Components of Instruments:** Some instruments produce significant energy in the high frequencies that are not related to simple harmonic series (e.g., the sizzle of a cymbal).\n",
        "* **Need More Context:** To definitively say whether the content above 3 kHz is harmonics, you'd need more information:\n",
        "    * **What is the audio source?** (e.g., a single instrument, speech, a mix of sounds)\n",
        "    * **Does it contain pitched sounds?**\n",
        "    * **Is there a clear fundamental frequency visible in the plot?**\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "While frequencies above 3 kHz *can* represent harmonics, they can also be other things. You need to analyze the overall plot and have context about the audio to make a more accurate determination."
      ],
      "metadata": {
        "id": "JCmrPDk5pzAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from IPython.display import Audio\n",
        "\n",
        "# def generate_tone(freq, sr, duration, amp=1.0):\n",
        "#     \"\"\"\n",
        "#     Generates a sinusoidal tone.\n",
        "\n",
        "#     Args:\n",
        "#         freq (float): Frequency of the tone in Hz.\n",
        "#         sr (int): Sampling rate in Hz.\n",
        "#         duration (float): Duration of the tone in seconds.\n",
        "#         amp (float): Amplitude of the tone (0.0 to 1.0).\n",
        "\n",
        "#     Returns:\n",
        "#         numpy.ndarray: The generated tone as a numpy array.\n",
        "#     \"\"\"\n",
        "#     time = np.linspace(0, duration, int(sr * duration), False)  # Time array\n",
        "#     tone = amp * np.sin(2 * np.pi * freq * time)\n",
        "#     return tone\n",
        "\n",
        "# # Parameters for the 1kHz tone\n",
        "# freq = 440  # Hz\n",
        "# sr = 44100  # Standard CD quality sampling rate\n",
        "# duration = 2.0  # seconds\n",
        "# amplitude = 0.5  # Reduce amplitude to avoid clipping\n",
        "\n",
        "# # Generate the tone\n",
        "# tone_signal = generate_tone(freq, sr, duration, amplitude)\n",
        "\n",
        "# # Play the tone in the notebook\n",
        "# Audio(tone_signal, rate=sr)"
      ],
      "metadata": {
        "id": "T-sOSFwmrExu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from IPython.display import Audio\n",
        "\n",
        "# def generate_tone(freqs, sr, duration, amps=None):\n",
        "#     \"\"\"\n",
        "#     Generates a tone with multiple frequencies (harmonics).\n",
        "\n",
        "#     Args:\n",
        "#         freqs (list or numpy.ndarray): List or array of frequencies in Hz.\n",
        "#         sr (int): Sampling rate in Hz.\n",
        "#         duration (float): Duration of the tone in seconds.\n",
        "#         amps (list or numpy.ndarray, optional): List or array of amplitudes\n",
        "#               for each frequency. Must be the same length as freqs.\n",
        "#               If None, all amplitudes are set to 1.0.\n",
        "\n",
        "#     Returns:\n",
        "#         numpy.ndarray: The generated tone as a numpy array.\n",
        "#     \"\"\"\n",
        "#     time = np.linspace(0, duration, int(sr * duration), False)\n",
        "#     tone = np.zeros_like(time)  # Initialize with zeros\n",
        "\n",
        "#     if amps is None:\n",
        "#         amps = np.ones_like(freqs)  # Default amplitudes\n",
        "\n",
        "#     for i, freq in enumerate(freqs):\n",
        "#         tone += amps[i] * np.sin(2 * np.pi * freq * time)\n",
        "\n",
        "#     # Normalize to avoid clipping (optional, but recommended)\n",
        "#     tone /= np.max(np.abs(tone))\n",
        "#     return tone\n",
        "\n",
        "# # Parameters for the tone with harmonics\n",
        "# fundamental_freq = 440  # A4 note\n",
        "# harmonic_frequencies = [\n",
        "#     fundamental_freq,            # 1st harmonic (fundamental)\n",
        "#     2 * fundamental_freq,        # 2nd harmonic (octave)\n",
        "#     3 * fundamental_freq,        # 3rd harmonic\n",
        "#     4 * fundamental_freq         # 4th harmonic (2nd octave)\n",
        "# ]\n",
        "# harmonic_amplitudes = [\n",
        "#     0.5,                       # Fundamental (relatively strong)\n",
        "#     0.25,                      # 2nd harmonic (weaker)\n",
        "#     0.125,                     # 3rd harmonic (even weaker)\n",
        "#     0.0625                     # 4th harmonic (weakest)\n",
        "# ]\n",
        "# sr = 44100\n",
        "# duration = 2.0\n",
        "\n",
        "# # Generate the tone with harmonics\n",
        "# tone_signal = generate_tone(harmonic_frequencies, sr, duration, harmonic_amplitudes)\n",
        "\n",
        "# # Play the tone\n",
        "# Audio(tone_signal, rate=sr)"
      ],
      "metadata": {
        "id": "MkYurtYQrxbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanations:**\n",
        "\n",
        "* **`generate_tone()` Function:**\n",
        "    * `freqs`: Takes a *list* or NumPy array of frequencies. This allows you to specify multiple frequencies (the fundamental and its harmonics).\n",
        "    * `amps`: An optional `amps` argument is added. It's a list or array of amplitudes, one for each frequency in `freqs`. If `amps` is not provided, all frequencies are assumed to have an amplitude of 1.0.\n",
        "    * `tone = np.zeros_like(time)`: Initializes the `tone` array with zeros. This is important because we'll be *adding* the sine waves for each harmonic to this array.\n",
        "    * The `for` loop calculates the sine wave for each harmonic and adds it to the `tone` array.\n",
        "    * `tone /= np.max(np.abs(tone))` : Normalizes the final `tone` signal. This is a good practice to prevent clipping (distortion) if the combined amplitudes of the harmonics make the signal too loud. It scales the entire signal down so that its maximum absolute value is 1.0.\n",
        "* **Example Usage:**\n",
        "    * `fundamental_freq`:  Sets the base frequency (e.g., 440 Hz for the A4 musical note).\n",
        "    * `harmonic_frequencies`:  A list defining the frequencies of the harmonics.  The frequencies are multiples of the fundamental.\n",
        "    * `harmonic_amplitudes`:  A list specifying the relative amplitudes of the harmonics. Notice how the amplitudes decrease for higher harmonics; this is common in many real-world sounds.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "The code generates a sine wave for each frequency in the `harmonic_frequencies` list and then adds these sine waves together. The `harmonic_amplitudes` control the relative strength of each harmonic, shaping the overall timbre of the sound."
      ],
      "metadata": {
        "id": "iOFi1chqsmx8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91c8346a-ad75-4d02-8f2e-829c81ff7c5d"
      },
      "source": [
        "## Spectrograms and Short-Time Fourier Transform\n",
        "\n",
        "A spectrogram relies on the third dimension of color to provide information about audio data. The x-axis represents time, and the y-axis represents frequency. The color in the spectrogram indicates the loudness (amplitude) of the audio at each point in time and frequency.\n",
        "\n",
        "The Short-Time Fourier Transform (STFT) is a mathematical technique used to generate spectrograms. It computes the Fourier Transform over short, overlapping time windows, allowing us to observe the evolution of frequencies over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ec6ca6-0c8c-4057-a7cc-030124d2c357"
      },
      "outputs": [],
      "source": [
        "# # Short-Time Fourier Transform (STFT)\n",
        "# D = librosa.stft(samples)\n",
        "\n",
        "# # Convert the amplitude to dB scale\n",
        "# D_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "# # Plot the STFT\n",
        "# plt.figure(figsize=(10, 2),dpi=150)\n",
        "# librosa.display.specshow(D_dB, sr=sr, x_axis='time', y_axis='log')\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('STFT Magnitude')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio(data=samples, rate=sr)"
      ],
      "metadata": {
        "id": "B-9T8ERpZ9Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Axes**\n",
        "\n",
        "* **X-axis (Time):** This axis represents time, measured in seconds. It shows the duration of the audio clip being analyzed. In this case, the audio appears to be around 8 seconds long.\n",
        "* **Y-axis (Hz):** This axis represents frequency, measured in Hertz (Hz). It shows the range of frequencies present in the audio, from 0 Hz up to 16384 Hz (16.384 kHz). The y-axis is often scaled linearly, as it is here.\n",
        "\n",
        "**Color Mapping (Amplitude/Intensity)**\n",
        "\n",
        "* **Color Bar:** The color bar on the right is crucial. It represents the magnitude (or intensity) of the frequencies at each point in time.\n",
        "    * **Warmer Colors (Yellow, Orange, Red):** Indicate higher magnitudes (louder or stronger presence of those frequencies).  The color bar shows that yellow is close to 0 dB, which is a relatively high amplitude.\n",
        "    * **Cooler Colors (Purple, Black):** Indicate lower magnitudes (quieter or weaker presence of those frequencies).  Black is at -80 dB, representing a very low amplitude.\n",
        "\n",
        "**Interpreting the Spectrogram**\n",
        "\n",
        "* **Frequency Content Over Time:** The spectrogram shows the frequency \"fingerprint\" of the audio as it unfolds. You can see which frequencies are present and how their strength changes over the duration of the sound.\n",
        "* **Horizontal Bands:** Horizontal bands (or lines) indicate sustained frequencies. If you see a bright horizontal line, it means that frequency is consistently present.\n",
        "* **Vertical Patterns:** Vertical patterns indicate changes that occur across a range of frequencies at a specific moment in time. These can represent transient events like percussive hits, plosives in speech, or sudden changes in musical chords.\n",
        "* **Silence/Quiet Sections:** Dark regions (black or dark purple) indicate periods where there is little or no energy at those frequencies (silence or very quiet sounds).\n",
        "\n",
        "**Specific Observations (Based on the Image)**\n",
        "\n",
        "* **Overall Frequency Range:** The audio appears to have significant energy across a broad frequency range, from low frequencies up to higher frequencies.\n",
        "* **Time Variations:** There are clear changes in the frequency content over time.  For example, there are distinct vertical structures at roughly 1-second intervals, suggesting rhythmic or repeated events.\n",
        "* **Low-Frequency Emphasis:** There's a good amount of energy in the lower frequencies (below 1000 Hz), which might indicate bass or fundamental tones.\n",
        "* **Transient Events:** The sharp vertical lines suggest short, broadband events occurring throughout the audio. These could be clicks, pops, or percussive sounds.\n",
        "* **Possible Harmonic Structure:** The presence of some horizontal lines, especially in the lower frequencies, hints at possible harmonic content (like from musical instruments or pitched sounds).\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "This spectrogram provides a detailed visual representation of the audio signal's frequency content over time. By analyzing the color patterns, you can gain insights into the various sounds present, their relative loudness, and how they evolve throughout the recording. It's a powerful tool for audio analysis and understanding the characteristics of sound."
      ],
      "metadata": {
        "id": "FmgL03SQaZ7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEL Spectogram\n",
        "\n",
        "A Mel spectrogram is a way of visualizing audio data that is similar to a standard spectrogram, but with some key differences in how the frequencies are represented. The Mel scale is a perceptual scale of pitches that is designed to mimic how human ears respond to different frequencies."
      ],
      "metadata": {
        "id": "Opd-QP_Bogf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2bc3cf3-91ac-4c3b-8a8d-62ab3d114d9f"
      },
      "outputs": [],
      "source": [
        "# S = librosa.feature.melspectrogram(y=samples, sr=sr, n_mels=128, fmax=8000)\n",
        "# S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "# plt.figure(figsize=(10, 2),dpi=150)\n",
        "# librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sr, fmax=8000)\n",
        "# plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context (from the Code)**\n",
        "\n",
        "* The code snippet at the top provides important context:\n",
        "    * `s = librosa.feature.melspectrogram(y=samples, sr=sr, n_mels=128, fmax=8000)`: This line calculates the Mel spectrogram using the `librosa` library.\n",
        "        * `y=samples`:  The audio data itself.\n",
        "        * `sr=sr`: The sampling rate of the audio.\n",
        "        * `n_mels=128`:  The number of Mel bands used (determines the vertical resolution of the plot).\n",
        "        * `fmax=8000`:  The maximum frequency displayed on the y-axis (8000 Hz or 8 kHz). This is a key difference from a regular spectrogram, where `fmax` might be higher.\n",
        "    * `s_db = librosa.power_to_db(s, ref=np.max)`:  This converts the power of the Mel spectrogram to decibels (dB), which is a more perceptually relevant scale for loudness. `ref=np.max` sets the reference to the maximum value in the spectrogram.\n",
        "    * The rest of the code sets up the plot using `matplotlib` and `librosa.display.specshow`.\n",
        "\n",
        "**Axes**\n",
        "\n",
        "* **X-axis (Time):** Represents time in seconds. It shows the duration of the audio clip.\n",
        "* **Y-axis (Hz or \"mel\"):** This is where the key difference lies. While it's labeled \"Hz\" in the image, it's really representing the **Mel scale**.\n",
        "    * The Mel scale is a transformation of the frequency scale designed to approximate how the human ear perceives pitch. It's more linear in pitch perception than the Hertz scale, especially at higher frequencies.\n",
        "    * Even though the units are still displayed in Hz (4096, 2048, etc.), these values are the *Hertz values that correspond to specific points on the Mel scale*.\n",
        "    * The important thing is that the spacing between the numbers on the y-axis is not linear in Hz but is designed to be more perceptually uniform.\n",
        "\n",
        "**Color Mapping (Amplitude/Intensity in dB)**\n",
        "\n",
        "* **Color Bar:** The color bar on the right represents the magnitude (loudness) of the audio signal in decibels (dB).\n",
        "    * Warmer colors (yellow, orange, red) indicate higher dB values (louder sounds).\n",
        "    * Cooler colors (purple, black) indicate lower dB values (quieter sounds).\n",
        "\n",
        "**Interpreting the Mel Spectrogram**\n",
        "\n",
        "* **Human Pitch Perception:** The main reason for using a Mel spectrogram is that it's often more relevant for audio analysis tasks related to human hearing, such as:\n",
        "    * Speech recognition\n",
        "    * Music analysis\n",
        "    * Audio classification\n",
        "* **Frequency Resolution Changes:** Compared to a regular spectrogram, the Mel spectrogram has a different frequency resolution. It generally has:\n",
        "    * Finer resolution at lower frequencies (more detail in bass and lower midrange)\n",
        "    * Coarser resolution at higher frequencies (less detail in treble)\n",
        "* **Simplified Representation:** It can sometimes provide a simplified representation of the audio, highlighting features that are perceptually important.\n",
        "\n",
        "**Specific Observations (Based on the Image)**\n",
        "\n",
        "* **Limited Frequency Range:** The plot only goes up to 4096 Hz (4 kHz) on the Mel scale, as set by `fmax=8000` in the code (note that the y-axis labels show Hz values, but they correspond to Mel scale values).\n",
        "* **Time Structure:** Like the regular spectrogram, there are distinct vertical structures suggesting changes in audio over time.\n",
        "* **Energy Distribution:** You can observe the distribution of energy across the Mel bands.\n",
        "* **Comparison to Regular Spectrogram:** If you compared this Mel spectrogram to a regular spectrogram of the same audio, you'd likely see:\n",
        "    * Different spacing of the frequency information on the y-axis.\n",
        "    * Potentially a different emphasis on certain frequency ranges.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "This Mel spectrogram is a visual representation of audio where the frequency scale is transformed to the Mel scale to better align with human pitch perception. It's particularly useful for tasks where understanding how humans perceive the audio is crucial. The color mapping and the x-axis (time) are interpreted similarly to a regular spectrogram, but the y-axis represents Mel-scaled frequencies."
      ],
      "metadata": {
        "id": "k2HkFqlbawrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's the difference between a regular spectrogram and a Mel spectrogram?\n",
        "\n",
        "Here's a breakdown of their key differences:\n",
        "\n",
        "**1. Frequency Scale**\n",
        "\n",
        "* **Regular Spectrogram:**\n",
        "    * Uses a linear frequency scale (y-axis is in Hertz - Hz).\n",
        "    * This means that the frequency intervals are evenly spaced. For example, the distance between 100 Hz and 200 Hz is the same as the distance between 1000 Hz and 1100 Hz.\n",
        "    * This representation is accurate for showing the physical frequencies present in the audio signal.\n",
        "* **Mel Spectrogram:**\n",
        "    * Uses the Mel scale on the y-axis.\n",
        "    * The Mel scale is a transformation of the Hertz scale designed to approximate how the human ear perceives pitch.\n",
        "    * It's non-linear, with finer resolution at lower frequencies and coarser resolution at higher frequencies.\n",
        "    * The spacing between Mel frequency bands is designed to be perceptually more uniform, meaning equal intervals on the Mel scale correspond to roughly equal intervals in perceived pitch.\n",
        "\n",
        "**2. Perceptual Relevance**\n",
        "\n",
        "* **Regular Spectrogram:**\n",
        "    * Represents the raw frequency content of the audio signal accurately.\n",
        "    * May not be the most suitable for tasks where human pitch perception is crucial.\n",
        "* **Mel Spectrogram:**\n",
        "    * Is more aligned with human hearing, making it advantageous for tasks like:\n",
        "        * Speech recognition\n",
        "        * Music analysis\n",
        "        * Audio classification\n",
        "\n",
        "**3. Visual Representation**\n",
        "\n",
        "* **Regular Spectrogram:**\n",
        "    * Provides a more detailed view of the higher frequency components.\n",
        "    * May show less detail in the lower frequency range compared to a Mel spectrogram.\n",
        "* **Mel Spectrogram:**\n",
        "    * Emphasizes the lower frequencies, which are often perceptually more important.\n",
        "    * May appear \"compressed\" at higher frequencies due to the coarser resolution.\n",
        "\n",
        "**In Simple Terms**\n",
        "\n",
        "Imagine looking at a painting. A regular spectrogram is like a photograph, capturing every detail of the colors. A Mel spectrogram is like a painting where the colors are adjusted to match how your eyes perceive them â€“ some colors are more emphasized, and others are less detailed.\n",
        "\n",
        "The choice between a regular spectrogram and a Mel spectrogram depends on the specific audio analysis task. If you need precise frequency information, use a regular spectrogram. If you're working with human perception, a Mel spectrogram is often a better choice."
      ],
      "metadata": {
        "id": "jr258tw1dacf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Classification"
      ],
      "metadata": {
        "id": "lykFBfTia__u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, transformers, torchaudio\n",
        "\n",
        "# print(torch.__version__)\n",
        "# print(torchaudio.__version__)\n",
        "# print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "S6-IjPRhbC05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
        "\n",
        "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
      ],
      "metadata": {
        "id": "yQlKsykbbcIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = feature_extractor(samples,return_tensors=\"pt\")\n",
        "# model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "# prediction_logits = model(result['input_values']).logits\n",
        "# predicted_class_ids = torch.argmax(prediction_logits, dim=-1).item()\n",
        "# predicted_label = model.config.id2label[predicted_class_ids]\n",
        "# predicted_label"
      ],
      "metadata": {
        "id": "nGgtKwYjbte3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# pipe = pipeline(\"audio-classification\", model=\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "# pipe('/content/drive/MyDrive/AI Tools/Hugging Face/Audio/audio.mp3')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "afkeZb1bcP6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Generation"
      ],
      "metadata": {
        "id": "6vg9buFbf0AC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f11a49ca-9893-425f-8624-f555a7b5f6b8"
      },
      "outputs": [],
      "source": [
        "# import transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb3205cb-6400-4270-9ffe-ba5b4c0d1cf8"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# pipe = pipeline(\"text-to-speech\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbc00ab4-25d0-4476-9e61-4abb2dd112c4"
      },
      "outputs": [],
      "source": [
        "# text = \"Today is a good day to learn about Hugging Face audio?\"\n",
        "# output = pipe(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4ac9c5-f50a-44c9-9d35-4435a6939d1e"
      },
      "outputs": [],
      "source": [
        "# output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11354dcb-430a-41a8-b2a0-a84bcbca0eee"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Audio\n",
        "\n",
        "# # Play the loaded audio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terms\n",
        "\n",
        "Here is an alphabetized list of related terms with their explanations:\n",
        "* Amplitude: Describes sound pressure level at any given instant, and that's measured in decibels. From a human perspective, that is perceived as loudness.\n",
        "\n",
        "\n",
        "* Amplitude spectrum: Representation of the magnitude of different frequency components in a signal.\n",
        "\n",
        "\n",
        "* Argmax: A function that finds the index of the maximum value in a set of data.\n",
        "\n",
        "\n",
        "* Atmospheric pressure level: The force exerted by the weight of air in the atmosphere.\n",
        "\n",
        "\n",
        "* Audio classification: Categorizing audio into different types.\n",
        "\n",
        "\n",
        "* Audio data: Information represented as sound.\n",
        "\n",
        "\n",
        "* Audio file: A digital recording of sound.\n",
        "\n",
        "\n",
        "* Audio generation: Creating new audio using machine learning models.\n",
        "\n",
        "\n",
        "* Audio transcription: Converting audio into text.\n",
        "\n",
        "\n",
        "* Automatic feature extraction: The process of automatically extracting relevant features from raw data.\n",
        "\n",
        "\n",
        "* Automatic speech recognition: Using machine learning to transcribe spoken language into text automatically.\n",
        "\n",
        "\n",
        "* Bit depth: Determines the precision of amplitude values in digital audio.\n",
        "\n",
        "\n",
        "* Consolidated segments: Merged audio segments from the same speaker.\n",
        "\n",
        "\n",
        "* Decibels (dB): Units used to measure the amplitude (loudness) of a sound.\n",
        "\n",
        "\n",
        "* Diarization: The process of determining who is speaking when in an audio recording.\n",
        "\n",
        "\n",
        "* Digital audio formatting: How audio is represented in a digital format.\n",
        "\n",
        "\n",
        "* Digital audio samples: Discrete values representing the amplitude of an audio wave at a specific point in time.\n",
        "\n",
        "\n",
        "* Discrete Fourier Transform (DFT): A mathematical technique used to transform a signal from the time domain to the frequency domain.\n",
        "\n",
        "\n",
        "* Fast Fourier Transform (FFT): An algorithm for efficiently computing the Discrete Fourier Transform.\n",
        "\n",
        "\n",
        "* Frequency: The rate at which sound waves vibrate, measured in Hertz (Hz).\n",
        "\n",
        "\n",
        "* Frequency domain: Representation of a signal by its constituent frequencies.\n",
        "\n",
        "\n",
        "* Frequency spectrum: A visual representation of the amplitude of an audio signal across different frequencies.\n",
        "\n",
        "\n",
        "* Gated models: Machine learning models that require permission to access.\n",
        "\n",
        "\n",
        "* Hertz (Hz): The unit of frequency, equal to one cycle per second.\n",
        "\n",
        "\n",
        "* Hugging Face: A platform that provides various open-source models that can be used for audio-based tasks.\n",
        "\n",
        "\n",
        "* iPython: Interactive computing environment.\n",
        "\n",
        "\n",
        "* Iter tracks: A method to iterate through different speakers in an audio file.\n",
        "\n",
        "\n",
        "* Kilohertz (kHz): A unit of frequency equal to 1,000 hertz.\n",
        "\n",
        "\n",
        "* Librosa: A Python library specifically designed for working with audio data.\n",
        "\n",
        "\n",
        "* Logarithmic: Relating to a scale of measurement in which an increase of one unit represents a multiplication by a fixed factor.\n",
        "\n",
        "\n",
        "* Logits: Raw, unnormalized predictions from a classification model.\n",
        "\n",
        "\n",
        "* Matplotlib: A Python library used for creating visualizations.\n",
        "\n",
        "\n",
        "* Mel scale: A perceptual scale of pitches that human listeners judge to be equal in distance from one another.\n",
        "\n",
        "\n",
        "* Mel spectrogram: A spectrogram where the frequency scale is transformed to the Mel scale, which aligns more closely with human pitch perception.\n",
        "\n",
        "\n",
        "* Modality: A method of doing something.\n",
        "\n",
        "\n",
        "* Numpy: A Python library used for working with arrays.\n",
        "\n",
        "\n",
        "* Piano: A library used for speaker diarization.\n",
        "\n",
        "\n",
        "* Pi Dub: A Python library that facilitates audio file manipulation.\n",
        "\n",
        "\n",
        "* Pipeline: A tool that simplifies the process of using machine learning models for specific tasks.\n",
        "\n",
        "\n",
        "* Pre-trained model: A model that has been trained on a large dataset and can be used for transfer learning.\n",
        "\n",
        "\n",
        "* Python: A programming language used to process and visualize audio data.\n",
        "\n",
        "\n",
        "* Quantization noise: Rounding off continuous values to discrete values, which can reduce audio quality.\n",
        "\n",
        "\n",
        "* Resample/Down sample: Convert from a higher to a lower sampling rate.\n",
        "\n",
        "\n",
        "* Sampling rate: The number of measurements taken per second when recording audio, often measured in kilohertz (kHz).\n",
        "\n",
        "\n",
        "* Sentiment analysis: The process of determining the emotional tone behind a piece of text.\n",
        "\n",
        "\n",
        "* Short Time Fourier Transform (STFT): A technique to analyze how the frequency content of a signal changes over time.\n",
        "\n",
        "\n",
        "* Softmax: A function that converts raw scores (logits) into probabilities.\n",
        "\n",
        "\n",
        "* Sound wave: The pattern of disturbance caused by the movement of energy traveling through a medium (like air).\n",
        "\n",
        "\n",
        "* Speaker: A device that converts energy from one form to another (e.g., a speaker).\n",
        "\n",
        "\n",
        "* Spectral leakage: An artifact that occurs in signal processing when frequencies in a signal appear at incorrect frequencies in the spectrum.\n",
        "\n",
        "\n",
        "* Spectrogram: A visual representation of audio that displays time, frequency, and amplitude (loudness).\n",
        "\n",
        "\n",
        "* Text to speech: The process of converting text into spoken words using machine learning.\n",
        "\n",
        "\n",
        "* Time domain: Representation of a signal as it evolves over time.\n",
        "\n",
        "\n",
        "* Timestamped transcription: A transcription that includes timestamps indicating when each word was spoken.\n",
        "\n",
        "\n",
        "* Torch: An open-source machine learning framework.\n",
        "\n",
        "\n",
        "* Torch audio: A library that provides audio-specific functionalities for PyTorch.\n",
        "\n",
        "\n",
        "* Transducer: A device that converts energy from one form to another (e.g., a speaker).\n",
        "\n",
        "\n",
        "* Transformers: A library used for various machine learning tasks, including audio classification.\n",
        "\n",
        "\n",
        "* Unsupervised fashion: A type of machine learning where the algorithm learns patterns from unlabeled data.\n",
        "\n",
        "\n",
        "* Waveform: A visual representation of an audio signal, showing amplitude over time.\n",
        "\n",
        "\n",
        "* Window function: A function used to minimize spectral leakage in the Fourier transform.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Qa2pQsVDrRI"
      }
    }
  ]
}