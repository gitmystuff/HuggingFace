{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/HuggingFace/blob/main/AutoTokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer\n",
        "\n",
        "* https://dev.to/ajmal_hasan/using-hugging-face-models-in-google-colab-a-beginners-35ll"
      ],
      "metadata": {
        "id": "PwYLBs-3WS0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffuser Sidetrack"
      ],
      "metadata": {
        "id": "3gRpcTCrcQaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switch to GPU"
      ],
      "metadata": {
        "id": "q7TXewuy_TWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  !nvidia-smi"
      ],
      "metadata": {
        "id": "amAAklW7_Ypv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TnxugjXkWMBg"
      },
      "outputs": [],
      "source": [
        "# from diffusers import StableDiffusionPipeline\n",
        "# import torch\n",
        "\n",
        "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "# pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# prompt = \"Cat playing upright bass\"\n",
        "# image = pipe(prompt).images[0]\n",
        "# image.save(\"cat.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "hDFwvrFJ3Zzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install datasets"
      ],
      "metadata": {
        "id": "8tchc-kj4IAD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data"
      ],
      "metadata": {
        "id": "NN92MkLz3dXt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create df"
      ],
      "metadata": {
        "id": "KQ6QAXlm5OZC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment analysis pipeline"
      ],
      "metadata": {
        "id": "kjFy1jWo5ko6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sa example"
      ],
      "metadata": {
        "id": "D19XrUEj6VPb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels == 0"
      ],
      "metadata": {
        "id": "z72lILar8_ld"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def classify_sentiment(row):\n",
        "#     label = sa(row)[0]['label']\n",
        "#     if label == 'POSITIVE':\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 0\n"
      ],
      "metadata": {
        "id": "NSO8dcUA6lBD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy"
      ],
      "metadata": {
        "id": "6qgfnRsw7lm6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens and Word Embeddings\n",
        "\n",
        "* https://medium.com/data-science-collective/modern-nlp-tokenization-embedding-and-text-classification-448826f489bf\n",
        "\n",
        "In the context of Hugging Face, a tokenizer is a crucial component of the natural language processing (NLP) pipeline. Here's a breakdown:\n",
        "\n",
        "* **Purpose:**\n",
        "    * Tokenizers are responsible for converting raw text into a format that machine learning models can understand. Since models work with numbers, tokenizers translate text into numerical representations.\n",
        "    * Essentially, they break down text into smaller units (tokens) and then map those tokens to numerical IDs.\n",
        "\n",
        "* **Key Functions:**\n",
        "    * **Tokenization:** Splitting text into meaningful units (words, subwords, or characters).\n",
        "    * **Vocabulary Creation:** Establishing a mapping between tokens and their corresponding numerical IDs.\n",
        "    * **Preprocessing:** Handling tasks like normalization (e.g., lowercase conversion), truncation, and padding.\n",
        "    * **Adding Special Tokens:** Incorporating tokens that have special meanings for models (e.g., `[CLS]`, `[SEP]`, `[UNK]`).\n",
        "\n",
        "* **Hugging Face's Role:**\n",
        "    * The Hugging Face `tokenizers` library provides efficient and versatile implementations of various tokenization algorithms.\n",
        "    * It's designed for performance, making it suitable for both research and production environments.\n",
        "    * Hugging face also provides pretrained tokenizers that are designed to work with their pretrained models. This ensures that the text is processed in the same way that the model was trained on.\n",
        "\n",
        "* **Importance:**\n",
        "    * Tokenization is a fundamental step in NLP, as it directly impacts the quality of the model's input.\n",
        "    * Choosing the right tokenizer is essential for achieving optimal performance.\n",
        "\n",
        "In essence, a Hugging Face tokenizer bridges the gap between human language and machine understanding, enabling NLP models to effectively process and analyze text.\n"
      ],
      "metadata": {
        "id": "SL2mun-HwPEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text -> Tokenization -> Input IDs (Numerical Representation) -> Word Embeddings (Vectors)\n",
        "\n",
        "1.  **Tokenization:**\n",
        "    * This is the initial step where raw text is broken down into smaller, meaningful units called \"tokens.\" These tokens can be words, subwords (like parts of words), or even individual characters.\n",
        "    * The specific method of tokenization can vary (e.g., word-based, subword-based like Byte-Pair Encoding (BPE) or WordPiece).\n",
        "    * Hugging Face tokenizers handle this step efficiently.\n",
        "\n",
        "2.  **Conversion to Input IDs:**\n",
        "    * Once the text is tokenized, each token is then converted into a numerical representation. This is done by looking up the token in the tokenizer's vocabulary, which assigns a unique ID to each token.\n",
        "    * This results in a sequence of numbers, called input IDs, that the model can understand.\n",
        "\n",
        "3.  **Word Embeddings (Vectorization):**\n",
        "    * These input IDs are then fed into an embedding layer within the neural network.\n",
        "    * The embedding layer transforms each input ID into a dense vector of real numbers, known as a word embedding.\n",
        "    * These embeddings capture the semantic meaning of the tokens, allowing the model to understand relationships between words.\n",
        "    * So the input ID's are the index that is used to look up the corresponding vector within the embedding matrix.\n",
        "\n",
        "Therefore, to summarize:\n",
        "\n",
        "* Text -> Tokenization -> Input IDs (Numerical Representation) -> Word Embeddings (Vectors)\n",
        "\n",
        "This process allows NLP models to effectively process and understand textual data.\n"
      ],
      "metadata": {
        "id": "KudCK2EjfaBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bert"
      ],
      "metadata": {
        "id": "AQ1ft-LD4uZS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer"
      ],
      "metadata": {
        "id": "asxBPaIl6rFl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens"
      ],
      "metadata": {
        "id": "UbC8QVGc7O38"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi line"
      ],
      "metadata": {
        "id": "fDLseVFi8sGn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `input_ids`**\n",
        "\n",
        "* **What it is:**\n",
        "    * This is the core output. It's a list of lists, where each inner list represents a sentence from your `data` list.\n",
        "    * Each number within those inner lists is the numerical ID of a token from the BERT vocabulary.\n",
        "* **Explanation of the numbers:**\n",
        "    * `101` represents the `[CLS]` token (classification token). It's added at the beginning of each sentence and is often used for tasks like sentence classification.\n",
        "    * `102` represents the `[SEP]` token (separator token). It's added at the end of each sentence, signifying the end.\n",
        "    * The numbers in between (e.g., `1045`, `2066`, `8870`) are the IDs of the words or subwords that were tokenized from your sentences.\n",
        "    * For example:\n",
        "        * `1045` corresponds to \"i\"\n",
        "        * `2066` corresponds to \"like\"\n",
        "        * `8870` corresponds to \"cats\"\n",
        "    * The tokenizer's vocabulary maps each of these numbers back to their corresponding tokens.\n",
        "* **Why it's important:**\n",
        "    * These `input_ids` are the direct numerical representation of your text that the BERT model (or any transformer-based model) uses as input.\n",
        "\n",
        "**2. `token_type_ids`**\n",
        "\n",
        "* **What it is:**\n",
        "    * This is another list of lists, mirroring the structure of `input_ids`.\n",
        "    * It's used primarily for tasks involving multiple sentences (e.g., question answering, sentence pair classification).\n",
        "    * It indicates which segment each token belongs to.\n",
        "* **Explanation in your case:**\n",
        "    * In your example, all the `token_type_ids` are `0`. This is because you're only processing single sentences.\n",
        "    * If you were to provide two sentences to the tokenizer (e.g., a question and an answer), the `token_type_ids` would be used to distinguish between them. The first sentence would have `0`s, and the second sentence would have `1`s.\n",
        "* **Why it's important:**\n",
        "    * It helps the model understand the relationships between different segments of text.\n",
        "\n",
        "**3. `attention_mask`**\n",
        "\n",
        "* **What it is:**\n",
        "    * This is also a list of lists, with the same structure as `input_ids`.\n",
        "    * It's used to tell the model which tokens should be attended to and which should be ignored.\n",
        "* **Explanation in your case:**\n",
        "    * All the values in your `attention_mask` are `1`. This means that all the tokens are considered valid and should be attended to.\n",
        "    * The attention mask becomes particularly important when you're dealing with sequences of varying lengths. To handle this, tokenizers often pad shorter sequences to match the length of the longest sequence in a batch.\n",
        "    * When padding is added, the `attention_mask` is used to indicate which tokens are real (1) and which are padding (0). The model then ignores the padding tokens during attention.\n",
        "* **Why it's important:**\n",
        "    * It ensures that the model focuses on the relevant parts of the input and avoids being misled by padding tokens.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* `input_ids` are the numerical representation of your text.\n",
        "* `token_type_ids` are used to distinguish between different segments of text.\n",
        "* `attention_mask` is used to indicate which tokens should be attended to and which should be ignored (especially padding tokens).\n",
        "\n",
        "These outputs are essential for feeding your text data into a BERT model and allowing it to perform various NLP tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "7539tI8emh7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# import pprint\n",
        "\n",
        "# model = 'bert-base-uncased'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# sentence1 = \"What is the capital of France?\"\n",
        "# sentence2 = \"Paris is the capital.\"\n",
        "\n",
        "# encoded_input = tokenizer(sentence1, sentence2, padding=\"max_length\", max_length=15)\n",
        "# for k, v in encoded_input.items():\n",
        "#     print(k, v)\n",
        "\n",
        "# ids = encoded_input['input_ids']\n",
        "# print(len(ids))\n",
        "# pprint.pprint(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "FGRGWRR4eu7N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of Sentences"
      ],
      "metadata": {
        "id": "XDGejWwQvKyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = 'bert-base-uncased'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# sentence1 = \"What is the capital of France?\"\n",
        "# sentence2 = \"Paris is the capital.\"\n",
        "# sentence3 = \"My cats speak french better than i do.\"\n",
        "\n",
        "# sentences = [sentence1, sentence2, sentence3] # Put sentences into a list.\n",
        "\n",
        "# tokens = tokenizer(sentences, padding=\"max_length\", max_length=15, truncation=True)\n",
        "\n",
        "# for k, v in tokens.items():\n",
        "#     print(k, v)\n",
        "\n",
        "# ids = tokens['input_ids'][0] # Access the first sentence input ids.\n",
        "# print(len(ids))\n",
        "# pprint.pprint(tokenizer.decode(ids))\n",
        "\n",
        "# ids = tokens['input_ids'][1] # Access the first sentence input ids.\n",
        "# print(len(ids))\n",
        "# pprint.pprint(tokenizer.decode(ids))\n",
        "\n",
        "# ids = tokens['input_ids'][2] #Access the third sentence input ids.\n",
        "# print(len(ids))\n",
        "# pprint.pprint(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "IcW8MDWNoeUh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason the Hugging Face `tokenizer`, when used with multiple string arguments directly, is designed to handle *pairs* of sentences, not arbitrary lists, stems from the core architecture and training objectives of Transformer models like BERT. Here's a breakdown:\n",
        "\n",
        "**1. Next Sentence Prediction (NSP) in BERT:**\n",
        "\n",
        "* BERT, in its original pre-training, was trained with a task called \"Next Sentence Prediction\" (NSP).\n",
        "* The model was presented with pairs of sentences and had to predict whether the second sentence followed the first sentence in the original text.\n",
        "* This task was designed to help BERT understand relationships between sentences, which is crucial for tasks like question answering and natural language inference.\n",
        "* To facilitate NSP, the input to BERT was structured as two segments, separated by the `[SEP]` token.\n",
        "* The `token_type_ids` were used to distinguish between these two segments.\n",
        "\n",
        "**2. Tokenizer Design Reflects BERT's Training:**\n",
        "\n",
        "* The Hugging Face `tokenizer` was designed to be compatible with pre-trained models like BERT.\n",
        "* Therefore, its interface for handling multiple string arguments directly reflects the expected input format for BERT's NSP task.\n",
        "* When you provide two string arguments, the tokenizer assumes you're providing a sentence pair for a task that might involve sentence-level relationships.\n",
        "\n",
        "**3. Efficiency and Clarity:**\n",
        "\n",
        "* For tasks that specifically involve sentence pairs (e.g., question answering, paraphrase detection), this design provides a convenient way to prepare the input.\n",
        "* It's efficient because it directly produces the input format that BERT expects.\n",
        "* It also helps to make the code more readable when working with those types of tasks.\n",
        "\n",
        "**4. Handling Arbitrary Lists:**\n",
        "\n",
        "* For processing arbitrary lists of sentences, the `tokenizer` provides a different interface: passing a list of strings as the first argument.\n",
        "* This allows you to tokenize multiple independent sentences without assuming a sentence pair relationship.\n",
        "* This is the proper way to input many sentences into the tokenizer.\n",
        "\n",
        "**5. Evolution of Transformer Training:**\n",
        "\n",
        "* It's worth noting that the effectiveness of NSP has been debated, and some later Transformer models (like RoBERTa) have omitted it from their pre-training.\n",
        "* However, the `tokenizer`'s design still reflects the historical context of BERT's training.\n",
        "\n",
        "In essence, the tokenizer's behavior is a direct consequence of the training objectives and input format of the models it was designed to support. It prioritizes efficient handling of sentence pairs while also providing a separate mechanism for processing arbitrary lists of sentences.\n"
      ],
      "metadata": {
        "id": "wA0SfrCLwEnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colors by Allison"
      ],
      "metadata": {
        "id": "MPPrddYGCRui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "* Colab User... GenAI... Transformers O"
      ],
      "metadata": {
        "id": "MNhgrDN32GXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder vs Decoder"
      ],
      "metadata": {
        "id": "bs8qvkCk2Q8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT vs GPT3\n",
        "\n"
      ],
      "metadata": {
        "id": "icmVCAdywK9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shift from fine-tuning to few-shot learning.\n",
        "\n",
        "**1. \"The parameters are basically statistical weights about how text tends to flow\"**\n",
        "\n",
        "* **Explanation:**\n",
        "    * Large language models (LLMs) like GPT-3 are trained on vast amounts of text data.\n",
        "    * During training, they learn to predict the next word in a sequence.\n",
        "    * The \"parameters\" of the model are numerical values (weights) that determine the probability of each word appearing in a given context.\n",
        "    * Essentially, these parameters capture statistical patterns in the training data, reflecting how words and phrases tend to occur together.\n",
        "    * So, when you say \"text tends to flow,\" you're referring to the model's ability to generate text that follows these learned statistical patterns, creating coherent and seemingly natural language.\n",
        "\n",
        "**2. \"Different from BERT, GPT3 attempts to replace the downstream fine-tuning with few-shot learning.\"**\n",
        "\n",
        "* **BERT vs. GPT-3:**\n",
        "    * **BERT (Bidirectional Encoder Representations from Transformers):**\n",
        "        * BERT is primarily an encoder-based model, meaning it excels at understanding the context of words within a sentence.\n",
        "        * Traditionally, BERT is fine-tuned for specific downstream tasks (e.g., sentiment analysis, question answering) by adding a task-specific layer and training it on labeled data.\n",
        "    * **GPT-3 (Generative Pre-trained Transformer 3):**\n",
        "        * GPT-3 is a decoder-based model, designed for generating text.\n",
        "        * It was designed to demonstrate that with enough scale, a language model could perform various tasks with minimal or no task specific training.\n",
        "        * GPT3 aims to utilize in context learning, and minimize the need for fine tuning.\n",
        "\n",
        "**3. \"Few-Shot Learning (FSL) is a Machine Learning framework that enables a pre-trained model to generalize over new categories of data (that the pre-trained model has not seen during training) using only a few labeled samples per class.\"**\n",
        "\n",
        "* **Explanation:**\n",
        "    * FSL addresses the challenge of training models with limited labeled data.\n",
        "    * It allows a model to quickly adapt to new tasks or domains by providing only a small number of examples.\n",
        "    * This is particularly useful when collecting large labeled datasets is expensive or impractical.\n",
        "    * It is a form of meta learning.\n",
        "\n",
        "**4. \"GPT3 is given a small amount of data demonstration of the task at the inference time as conditioning but unlike the fine-tuning approach, there is no weight update. This is inspired by the fact that once humans have a general language understanding we donâ€™t necessarily need large supervised datasets to learn most language tasks.\"**\n",
        "\n",
        "* **In-Context Learning:**\n",
        "    * Instead of updating the model's weights through fine-tuning, GPT-3 uses \"in-context learning.\"\n",
        "    * This means that you provide a few examples of the task you want the model to perform directly in the input prompt.\n",
        "    * For example, you might give GPT-3 a few examples of translating English to French, and then ask it to translate a new sentence.\n",
        "    * The model uses these examples to understand the task and generate the desired output, without changing its underlying parameters.\n",
        "* **Human Analogy:**\n",
        "    * The approach is inspired by how humans learn. We can often grasp new concepts or tasks by seeing just a few examples, without needing extensive training.\n",
        "    * GPT-3's ability to perform few-shot learning suggests that it has learned a broad understanding of language and can apply that knowledge to new situations.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* GPT-3 represents a shift towards models that can perform a wide range of tasks with minimal task-specific training.\n",
        "* Few-shot learning allows these models to quickly adapt to new tasks by providing examples in the input prompt.\n",
        "* This approach is more flexible and efficient than traditional fine-tuning, and it reflects a more human-like way of learning.\n"
      ],
      "metadata": {
        "id": "iTMkjr__1w_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature, Top-K, Top-P\n",
        "\n",
        "* Temperature controls the probability selection, or adjusts the randomness of the distribution\n",
        "* Top-K sampling limits the selection to the top K tokens\n",
        "* Top-P sampling, or nucleus sampling, includes the most likely tokens whose cumulative probability exceeds a threshold"
      ],
      "metadata": {
        "id": "jr7OcL53-81v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative AI"
      ],
      "metadata": {
        "id": "oQuOd3tf-bS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch\n",
        "\n",
        "PyTorch is a very popular open-source machine learning framework. Essentially, it's a tool that helps developers and researchers build and train neural networks. Here's a breakdown of its key aspects:\n",
        "\n",
        "**Core Features:**\n",
        "\n",
        "* **Tensor Computation:**\n",
        "    * At its heart, PyTorch works with \"tensors,\" which are multi-dimensional arrays similar to NumPy arrays.\n",
        "    * It provides tools for performing various mathematical operations on these tensors, and crucially, it can accelerate these computations using GPUs (Graphics Processing Units), significantly speeding up machine learning tasks.\n",
        "* **Automatic Differentiation (Autograd):**\n",
        "    * A fundamental part of training neural networks involves calculating gradients. PyTorch's \"autograd\" feature automates this process.\n",
        "    * This means you don't have to manually derive complex mathematical formulas; PyTorch keeps track of the operations performed and calculates the gradients for you.\n",
        "* **Dynamic Computation Graphs:**\n",
        "    * PyTorch uses dynamic computation graphs, which provide flexibility and make it easier to debug models. This means the graph of operations is built as the code is executed, allowing for more intuitive and flexible development.\n",
        "* **Neural Network Modules (torch.nn):**\n",
        "    * PyTorch provides a set of pre-built neural network layers and functions within the `torch.nn` module. This simplifies the process of creating complex neural network architectures.\n",
        "* **Ease of Use and Flexibility:**\n",
        "    * PyTorch is known for its user-friendly interface and its close integration with the Python programming language. This makes it relatively easy to learn and use.\n",
        "    * Its flexibility makes it a favorite among researchers who need to experiment with novel neural network architectures.\n",
        "\n",
        "**Key Uses:**\n",
        "\n",
        "* Deep learning research\n",
        "* Computer vision (image recognition, etc.)\n",
        "* Natural language processing (text analysis, etc.)\n",
        "* And many other machine learning applications.\n",
        "\n",
        "In essence, PyTorch provides the tools and infrastructure needed to build and train powerful machine learning models.\n"
      ],
      "metadata": {
        "id": "JlFIpucFFQrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# next word setup"
      ],
      "metadata": {
        "id": "9Rh0-tcTAOcz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows the download progress of different components required for a language model, likely GPT-2 in this case. Here's a breakdown of what each file represents:\n",
        "\n",
        "* **`config.json`:**  This file contains the configuration settings for the model, such as the number of layers, attention heads, hidden units, and other architectural choices. It defines the structure and hyperparameters of the model.\n",
        "\n",
        "* **`model.safetensors`:** This file stores the learned weights (parameters) of the neural network. These weights determine how the model processes and generates text, and they are the result of the model's training on a massive dataset. The \".safetensors\" format is a newer format for saving PyTorch models that focuses on safety and portability.\n",
        "\n",
        "* **`generation_config.json`:** This file contains settings specifically related to text generation with the model. This might include parameters like `max_length`, `temperature` (controlling randomness), and `top_k` (limiting the choices for the next token).\n",
        "\n",
        "* **`tokenizer_config.json`:**  This file holds the configuration for the tokenizer associated with the model. It defines how the tokenizer splits text into tokens, maps tokens to numerical IDs, and handles special tokens.\n",
        "\n",
        "* **`vocab.json`:** This file contains the model's vocabulary, which is a list of all the tokens (words, subwords, or characters) the model knows. Each token is associated with a unique numerical ID.\n",
        "\n",
        "* **`merges.txt`:** This file is used by some tokenizers (like the one for GPT-2) to handle subword tokenization. It defines rules for merging subword units into full words.\n",
        "\n",
        "* **`tokenizer.json`:** This file might contain additional information or metadata related to the tokenizer.\n",
        "\n",
        "**In summary, these downloads represent the essential components that make up a language model:**\n",
        "\n",
        "* The model's architecture (`config.json`)\n",
        "* The learned knowledge (`model.safetensors`)\n",
        "* The text processing rules (`tokenizer_config.json`, `vocab.json`, `merges.txt`, `tokenizer.json`)\n",
        "* The generation parameters (`generation_config.json`)\n",
        "\n",
        "By downloading all these files, you get a complete and ready-to-use language model that can be loaded and used for various natural language processing tasks.\n"
      ],
      "metadata": {
        "id": "SDM8VHVxm_e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "WyfjxA7DwtNX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the output shape `torch.Size([1, 4, 50257])` that you're getting from your GPT-2 model:\n",
        "\n",
        "**Understanding the Output Shape**\n",
        "\n",
        "The output shape `torch.Size([1, 4, 50257])` represents the dimensions of the `logits` tensor produced by your GPT-2 model. Here's what each dimension signifies:\n",
        "\n",
        "* **Dimension 1: Batch Size (1)**\n",
        "    * The first dimension (with size 1) corresponds to the batch size. In this case, you're processing a single input sequence. If you were processing multiple sequences at once, this dimension would be equal to the number of sequences in your batch.\n",
        "* **Dimension 2: Sequence Length (4)**\n",
        "    * The second dimension (with size 4) represents the length of the input sequence. This means your `input_ids` tensor contains 4 tokens. GPT-2 processes these tokens sequentially, generating a probability distribution over possible next tokens at each step.\n",
        "* **Dimension 3: Vocabulary Size (50257)**\n",
        "    * The third dimension (with size 50257) corresponds to the size of GPT-2's vocabulary. This means the model is considering 50257 possible tokens that could follow the input sequence. The `logits` tensor contains a score (logit) for each of these possible tokens at each position in the sequence.\n",
        "\n",
        "**What are Logits?**\n",
        "\n",
        "* Logits are the raw, unnormalized scores output by the model before they are converted into probabilities using a softmax function.\n",
        "* Each logit represents how likely the model thinks a particular token is to be the next token in the sequence.\n",
        "* Higher logits indicate higher likelihood.\n",
        "\n",
        "**Visualizing the Output**\n",
        "\n",
        "You can think of the `logits` tensor as a 3D array:\n",
        "\n",
        "* The first dimension (batch size) is like having multiple of these 3D arrays.\n",
        "* The second dimension (sequence length) represents the different positions within the input sequence.\n",
        "* The third dimension (vocabulary size) contains a score for each possible token at each position.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Let's say your input sequence is \"The cat sat on\". The `logits` tensor would have:\n",
        "\n",
        "* 1st dimension: 1 (because you have one input sequence)\n",
        "* 2nd dimension: 4 (because \"The cat sat on\" has 4 tokens)\n",
        "* 3rd dimension: 50257 (the vocabulary size of GPT-2)\n",
        "\n",
        "For each of the 4 positions in the sequence, you'd have 50257 logits, representing the model's scores for each possible next token.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The output shape `torch.Size([1, 4, 50257])` indicates that your GPT-2 model has processed a single input sequence of length 4 and has produced a score for each of the 50257 possible next tokens at each position in the sequence. These scores (logits) can then be used to determine the most likely next token or to generate text."
      ],
      "metadata": {
        "id": "kyx0Zlu-B09D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logits"
      ],
      "metadata": {
        "id": "uJJ4Dlx0A7uj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final logits"
      ],
      "metadata": {
        "id": "-Xg5eIe6B6Ue"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top10"
      ],
      "metadata": {
        "id": "BpiZprW4Cb7M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax\n",
        "\n",
        "The softmax function is a crucial tool in machine learning, particularly in multi-class classification problems. Here's a breakdown of what it is and why it's used:\n",
        "\n",
        "**Core Function:**\n",
        "\n",
        "* **Converting Raw Scores to Probabilities:**\n",
        "    * The softmax function takes a vector of real numbers (often called \"logits\") as input and transforms them into a probability distribution. This means it outputs a vector of values where each value is between 0 and 1, and the sum of all the values is equal to 1.\n",
        "* **Multi-Class Classification:**\n",
        "    * It's primarily used in situations where you need to classify data into multiple distinct categories. For example:\n",
        "        * Identifying different types of animals in an image.\n",
        "        * Determining the topic of a text document.\n",
        "        * Predicting which word comes next in a sentence.\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "1.  **Exponentiation:**\n",
        "    * First, it takes the exponential of each input value. This ensures that all output values are positive.\n",
        "2.  **Normalization:**\n",
        "    * Then, it divides each exponentiated value by the sum of all the exponentiated values. This normalization step is what makes the output values sum to 1, creating a valid probability distribution.\n",
        "\n",
        "**Why It's Important:**\n",
        "\n",
        "* **Probabilistic Output:**\n",
        "    * Softmax provides a clear and interpretable output, giving you the probability of each class. This allows you to understand the model's confidence in its predictions.\n",
        "* **Facilitates Training:**\n",
        "    * Because softmax produces a differentiable output, it's compatible with gradient-based optimization algorithms used to train neural networks.\n",
        "* **Enables Comparisons:**\n",
        "    * By converting raw scores into probabilities, softmax makes it easy to compare the likelihood of different classes.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The softmax function is a way to take a set of arbitrary numbers and turn them into a set of probabilities, which is essential for many classification tasks in machine learning.\n"
      ],
      "metadata": {
        "id": "r_Rdw1ieFaCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top10 probabilities"
      ],
      "metadata": {
        "id": "05FenXqnCll8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilities sum"
      ],
      "metadata": {
        "id": "Edq_zzekCyM2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you see `tensor(0.9999, grad_fn=<AddBackward0>)`, here's a breakdown of what it means in the context of PyTorch:\n",
        "\n",
        "**1. `tensor(0.9999)`:**\n",
        "\n",
        "* This indicates the numerical value of the result. In this case, it's 0.9999.\n",
        "* This value represents the sum of the probabilities obtained after applying the softmax function to your `final_logits`.\n",
        "* Ideally, the sum of probabilities should be exactly 1.0. The slight deviation (0.9999) is due to floating-point precision limitations in computers.\n",
        "\n",
        "**2. `grad_fn=<AddBackward0>`:**\n",
        "\n",
        "* This part is related to PyTorch's automatic differentiation (autograd) system.\n",
        "* `grad_fn` stands for \"gradient function.\" It indicates the function that was used to compute this tensor, and it's used to calculate gradients during backpropagation.\n",
        "* `<AddBackward0>` specifically means that the tensor was produced by an addition operation. This is because the `torch.sum()` function, which you used, performs addition.\n",
        "* In essence, PyTorch is keeping track of the operations that were performed to create this tensor so that it can calculate gradients if needed. This is crucial for training neural networks, where gradients are used to update the model's weights.\n",
        "* In short it is saying that the operation that created the tensor was an addition operation, and that pytorch is able to calculate the gradient of that operation.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "* The `tensor(0.9999)` tells you that the sum of your probabilities is very close to 1, which is what you expect.\n",
        "* The `grad_fn` part is PyTorch's way of saying, \"I know how this number was calculated, and I can figure out how to adjust the inputs if needed.\"\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "* The numerical value (0.9999) is the result you're interested in for confirming that your probabilities are correctly normalized.\n",
        "* The `grad_fn` is PyTorch's internal bookkeeping for automatic differentiation, which is essential for training neural networks.\n"
      ],
      "metadata": {
        "id": "97F2VDFlFFRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum version 2"
      ],
      "metadata": {
        "id": "GA0uV_prEnj6"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}